--- tfstate is a very critical file, because this tfstate has the information of the resources. ANd whenever you do terraform apply, so this tf state file is checked and this is mathced with the the actual configuration and the terraform decides what to do.
--- Now what will happen if the tfstate file got deleted? And if you again do terrafrom apply, terraform will create all the resources again. This is since we have deleted the tfstate file and terraform doesn't have the info what is already existing.
--- SO, you have to store your tfstate file in save place. Now you have it on the laptop and what if the laptop gets crashed or stolen. 

--- In Your project there will be multiple people working in the team. 
--- If person A has written the terraform code and did terraform apply. Now he has tfstate file on his laptop and resources got created on aws. Now person B wants to update a resource, now if he even gets the code from person or maybe from github, made some chnages to the code and do terraform apply all resources will be created again not just update.
-- This is since terrafrom.tfstate file of person B does not have any reference of tfstate file of person A. So, this is not good.

--- So, this tfstate file must not be managed locally. It should be managed somewhere on remote.
--- If person A create some resources on aws the tfstate file on remote will be updated. Maybe store in s3 bucket. So, it is on cloud and is safe, also no need to worry about crash.
--- Now if person B make some modification and does terraform apply, the terrform knows that what is the state of the resources on aws as tfstate file is store on remote. And modify the resources accordingly.
--- Now this is called backend, if you store the tfstate file on local it is called local backend and if you store on remote it is called remote backend.
--- Remote backend can be anything S3 bucket, Azure block storage and you can store it on GitHub also. But we do not store in on GitHub, since it stores contains some secure information.
--- S3 is the most popular for remote backend.
--- Now you can delete your tfstate file from the local on VS code. Make sure you run terrafom destroy to make sure no resources are running. 
--- Now lets create remote backend. Lets use S3 for it.

--- Search terraform backend --> you select terraform docs and see what all kinds of backend are possible. 
--- We are going to use S3, for that we need to create a s3 bucket first. So, first create s3 bucket on aws with name 'terraform-tfstate' ---> you can give any name you want.
--- Enable bucket versioning ---> create bucket ---> our bucket got created.

--- Now we have created the bucket but need to tell terraform that what will be the remote backend. So that we need to do some configurations.
--- Now create a file called 'backend.tf' on you VS code.
--- Add the below code :

terraform {
  backend "S3"{
    bucket = "terraform-tfstate"
    key = "terraform.tfstate"
    region = " "
    encrypt = true
  }
}

--- So here we are creating a terraform block,with backend as S3.
--- Then the bucket name : "terraform-tfstate". Then you have to specify the name of the terrform state file in key, here we are giving it same 'terraform.tfstate' and if you want you can give a different name also.
--- In region we give the region in which our bucket is present. Then we have option to encrypt.
--- So we have said terrform that you no longer need to maintain the state file on local, maintain it on S3 as remote backend. In S3 bucket terraform-tfstate and in file terraform.tfstate.
-------------------------------

--- Now there is one more problem.
--- Now person A has some code which has a lot of resources which needs to be created, whih would take a lot of time, maybe 15 minutes. So he did terraform apply.
--- In the mean time while resources of person A were in process of getting created, person B also did terraform apply. 
--- So, while person A resources were getting created and tfstate file was getting udpdated and now the person B also does some apply at the same time parallely. 
--- NOw, the tfstate file is inconsistent, now both will try to modify the same tfstate file.
--- Now if there are 10 people in the team it would lead to inconsistency as sometimes more than 1 persons can try to apply at the same time. This will lead to inconsistency.
--- So, we do not want multiple people doing it at the same time. So what is the solution?
--- The solution is to lock the statefile. So, while one person operations are happening, we need lock the tfstate file.
--- This is called 'state locking'.
--- Now if we enable state locking if a person is doing terraform apply a lock will be acquired on this tfstate file.
--- So now in the meantime if any other person trys to do some operations it would give an error that the tfstate files is locked.
--- After the person A operation is done the lock will be released and person B can do his operations.
--- This will ensure that only one person can update the tfstate file at-a-time.

--- You have different ways to enable state locking. 
--- Before terraform 1.11 version, we used to maintain a seprate table for state locking.
--- In aws we have DynamoDB and DynamoDB could be used for state locking. So you can create a DynamoDB table. 
--- InDybamoDB ---> create a table with some name and metion that in your code in backend.tf file. So your code looks like : 

terraform {
  backend "S3"{
    bucket = "terraform-tfstate"
    key = "terraform.tfstate"
    region = " "
    encrypt = true
    dynamodb_table = "value"
  }
}

--- So your state locking will be managed using DynamoDB table. So, whenever a person does terraform apply a lock file will be created in the DynamoDb table and when the operation is over the lock file will expire.

--- Now this was working fine, but terraform came up with a version of why do we need to use a different resource just to manage the locking and why not S3 handles it. And that's what they did. 
--- Now, in terraform latest version you do not need to use DynamoDB table for state locking. If you want you can use there us that option. But terrraform has given you a good thing that, for just to maintan the statelocking you can use the same S3 bucket.
--- For that you just need to add the to the code in backend.tf file. The code would look like 
terraform {
  backend "S3"{
    bucket = "terraform-tfstate"
    key = "terraform.tfstate"
    region = " "
    encrypt = true
    use_lockfile = true
  }
}

--- So just use 'use_lockfile = true', so we are using S3 for locking. 
--- Still some projects are using DynamoDb and some are using S3 locking.
-----------------------

Now lets run the code :

--- Now, as we are updating the backend. Now we have made a change earlier our backend was local but now it is remote backend. So if you make change to your backend configuration, in that case you have to run terrfom init first.
--- terraform init
--- terraform validate 
--- terrafrom plan 
--- terraform apply 

--- Now you can see tfstate file is not getting created on local.
--- If you check the S3 buclet you can see statefile getting created and you can see lockfile. Since your operation is running so lock file got created to lock it. You can see the lockfile is gone once the apply is done.
--- You can see statefile in s3 bucket now.

--- terraform destroy
---------------------------------------------------

--- Now lets talk about terrafrom workspaces.
--- It is similar to namespaces in k8s. But here we call it as workspace.
--- Now I am writing a code to run on dev env, test env and prod env. I will not write a different code for each, I will maintain the same code. 
--- So, you are creating logical isolation for each env. 
--- Now might need some modifications to the code for different env. Like maybe we need t2.nano for dev, t2.large for test and t2.xlarge for prod. So, I need to have seprate terraform.tfvars files for each env with seperate configurations.

--- So we will create 3 different files.
--- First lets create a folder with name 'env-config'.
--- Now first rename the 'terraform.tfvars' file to 'dev.tfvars'. So this dev.tfvars files will have the values for our dev env.
--- copy the dev.tfvars file and create test.tfvars and prod.tfvars files in same env-config folder. You can make whatever changes you would like to make the the file. May be we just chnage the bucket name.
--- So, we have created 3 tfvars files for 3 different envs. ---> you can also create them in same folder or you can create a new folder, we are create env-config folder to make ot easier.
--- now if you do teeraform apply as below
--- terraform apply -var-file="env-config/dev.tfvars" ---> this will create the resources based on the configuration.
--- Now if we want to create resources for test env we havr to do :
--- terraform apply -var-file="env-config/test.tfvars" ---> this will create the resources based on the configuration.
--- But this will only modify the existing resources and will not create a new test env. So, it will only modify the existing resources.
--- This is since terraform doesn't know that whether we are create a prod env ot test env or prod env. For terraform it is just configuration values.
--- So that what we want only modify the existing resources? No, we want to create a new env.
--- In that case workspace will help us.

--- So, we will run terraform workspace commands.
--- terraform workspace show ---> this command will tell you in which workspaces you are in currently.
--- terraform workspace new dev ---> it will create dev env.
--- terraform workspace new test ---> it will create test env.
--- terraform workspace new prod ---> it will create prod env.

--- when you so terraform workspace new command it will switch you to that workspace once it is created.
--- terraform workspace show ---> this will show you in which workspace you are in.
--- terraform workspace select dev ---> this command will take you to the dev workspace. 
--- now from this dev env if I run the below command :
--- terraform apply -var-file="env-config/dev.tfvars" ---> So this will create resources based on the values of dev.tfvars.
--- Now if we go and check you statefile which is on S3 as remote backend and refresh it. You can see env:/ folder, if you get into it you can see 3 env folders with names 'dev/', 'test/' and 'prod/'.
--- If we open them dev/ folder you can see there is a tfstate file, you also have a tfstate file in test/ and also in prod/.So each env has its own tfstate.

--- Now we are in dev env, not if we want to deploy infra on test env.
--- terraform workspace select test
--- terraform apply -var-file="env-config/test.tfvars" ---> you can see it is creating fresh resources, it is not modifying it. This is beacuse we are doing it in test workspace.
--- So for test it is checking the tfstate file which is kept in test/ folder on s3. 
--- So seprate state files are being maintianed for seperate env.
----------------------------------

Modules:
--- Module is to make things re-usable.
--- Lets say we have 2 teams, team A and team B. Team A is working on APP-A service and team B on APP-B service.
--- Now APP-A needs infra, now the devops teams will write the terraform code which is required for creating resources. Like ec2 instance, S3 buckets, rds, IAM policies, network configs like VPC, Subnets and etc.
--- Now team of APP-B will also write the same stuff.
--- Now both are in same project and both are writing the complete code for creating the infra. Here code re-writing is happening. 
--- Which mean APP-A and APP-B both are writigng code for ec2 instance, s3, IAM and etc. So it would be better if we make things re-usable.

--- Now normally in DevOps projects, there would be a team called platform team/ Systems team. 
--- Now this team is reponsible for creating modules for different resources.
--- So they will write the code in modules, maybe a module to create ec2 instance, one more module for s3, another for IAM and etc.
--- Now for eg: a ec2 instance module will have full mode of creating a ec2 instance. there will be main.tf, outputs and etc. So, this module wil just create ec2 instance.
--- Same way s3 will have its module and same for IAM as well or etc.
--- Now the APP teams doesn't need to write the full code. If they want to create a ec2 instance they are going to source the module, and pass the varibales/values.
--- So APP-A and APP-B will just have a file with the vars or values to be passed to the module and then module does everything.
--- So, now APP-A or B teams are not writing the complete code they are just passing the required values and inputs and modules are creating the resources.
--- So we have inplemented re-usablilty.

--- Now this has one more benefit, which is about consistency. About maintaining the standards.
--- Now the platform team or systems team is reposibles for manging these modules and they will make these modules standardized. They will use all the good practices of creating resources like; ec2 instance, s3, IAm and etc. 
--- They will implement them in the modules only.
--- lets say tomorrow if a new change has come in a resource, maybe a new configuration of s3. Lets say S3 guves a new feature.
--- Now the code is written in the module. So the platform team will implement it in the module. Then these APP teams, they don't have to worry about it. They just have to use it.
-------------------------------

--- now lets see some more commands.
--- terraform workspace select dev
--- terraform state list ---> it gives you the list of resources which your tfstate file is managing. 
--- Now if you want to delete only one reource you can use the below command. Lets delete s3 bucket that we  have.
--- terraform destroy -target <name of the resource you would like to destory> ---> you can get he name of the resource by using the command --- terraform state list ----> using this command you can delete a specific resource using terraform.

--------------- End of class 3 of terraform.

